{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Model Evalutation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Augment training set images to ultimately improve models ability to generalise.\n",
        "* **Tackle business requirement 2**: create binary classification model to predict whether a cherry leaf is healthy or contains powdery mildew.\n",
        "* Validate model performance using validation dataset.\n",
        "* Test model performance using test model data.\n",
        "* Analyse model performance using a number of different metrics.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/cherry_leaves_dataset/cherry-leaves/train\n",
        "* inputs/cherry_leaves_dataset/cherry-leaves/validation\n",
        "* inputs/cherry_leaves_dataset/cherry-leaves/test\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Barplot and pie chart to visualise label and set distribution, respectively.\n",
        "* cherry_leaves_model\n",
        "* Metrics table, reporting on performance metrics such as precision, recall, f1-score.\n",
        "* Precision-recall plot\n",
        "* ROC (receiver operating characteristic) curve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Set input directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = 'inputs/cherry_leaves_dataset/cherry-leaves'\n",
        "train_dir = data_dir + '/train'\n",
        "val_dir = data_dir + '/validation'\n",
        "test_dir = data_dir + '/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set output directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "version_num = 'v1'\n",
        "output_dir = f'outputs/{version_num}'\n",
        "\n",
        "if os.path.exists(output_dir):\n",
        "    print('Output already exists - please pick a new version number if you want to create a new output directory')\n",
        "else:\n",
        "    os.makedirs(output_dir)\n",
        "    print(f'Output {version_num} created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = os.listdir(train_dir)\n",
        "print('Image labels are: ', labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Set image shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "image_shape = joblib.load(f'outputs/{version_num}/image_shape.pkl')\n",
        "image_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Plot number of images in each group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_counts, data_split, group = [], [], []\n",
        "\n",
        "for label in labels:\n",
        "    for i in ['train', 'test', 'validation']:\n",
        "        file_path = os.path.join(data_dir, i, label)\n",
        "        files = os.listdir(file_path)\n",
        "        num_files = len(files)\n",
        "        file_counts.append(num_files)\n",
        "        data_split.append(i)\n",
        "        group.append(label)\n",
        "        \n",
        "        print(f'* {i} - {label}: {len(os.listdir(file_path))} images')\n",
        "\n",
        "df = pd.DataFrame({'Set': data_split, 'Group': group, 'File_Counts': file_counts})\n",
        "\n",
        "sns.barplot(data=df, x='Set', y='File_Counts', hue='Group')\n",
        "plt.savefig(f'{output_dir}/labels_distribution.png')\n",
        "plt.title('Cherry leaves dataset distribution')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pie = df[0:3]\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.pie(df_pie['File_Counts'], labels=['Train', 'Test', 'Validation'], autopct='%1.f%%')\n",
        "plt.savefig(f'{output_dir}/set_distribution.png')\n",
        "plt.title('Pie chart of train, test, validation split')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Augment images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "augmented_img_data = ImageDataGenerator(rotation_range=20,\n",
        "                                        width_shift_range=0.2,\n",
        "                                        height_shift_range=0.2,\n",
        "                                        shear_range=0.1,\n",
        "                                        zoom_range=0.1,\n",
        "                                        horizontal_flip=True,\n",
        "                                        vertical_flip=True,\n",
        "                                        fill_mode='nearest',\n",
        "                                        rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augment train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_set = augmented_img_data.flow_from_directory(train_dir,\n",
        "                                                   target_size=image_shape[:2],\n",
        "                                                   color_mode='rgb',\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   class_mode='binary',\n",
        "                                                   shuffle=True\n",
        "                                                   )\n",
        "train_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Augment validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_set = ImageDataGenerator(rescale=1./255).flow_from_directory(val_dir,\n",
        "                                                   target_size=image_shape[:2],\n",
        "                                                   color_mode='rgb',\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   class_mode='binary',\n",
        "                                                   shuffle=False\n",
        "                                                   )\n",
        "val_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Augment test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set = ImageDataGenerator(rescale=1./255).flow_from_directory(test_dir,\n",
        "                                                   target_size=image_shape[:2],\n",
        "                                                   color_mode='rgb',\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   class_mode='binary',\n",
        "                                                   shuffle=False\n",
        "                                                   )\n",
        "test_set.class_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot augmented train and validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    plt.figure(figsize=(3,3))\n",
        "    img, label = train_set.next()\n",
        "    plt.imshow(img[i])\n",
        "    print(img.shape)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot augmented validation images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    plt.figure(figsize=(3,3))\n",
        "    img, label = val_set.next()\n",
        "    plt.imshow(img[i])\n",
        "    print(img.shape)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save class indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=train_set.class_indices,\n",
        "            filename=f\"{output_dir}/class_indices.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPooling2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def make_model():\n",
        "    \n",
        "    model = Sequential()\n",
        "\n",
        "    # input layer  \n",
        "    model.add(Conv2D(32, (3,3), activation='relu', input_shape=image_shape))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    \n",
        "    # hidden layers\n",
        "    model.add(Conv2D(32, (3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(2,2))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten()) # flattens array to 1D\n",
        "\n",
        "    # fully connected layer\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # output\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "make_model().summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = make_model()\n",
        "model.fit(train_set,\n",
        "          epochs=25,\n",
        "          steps_per_epoch = len(train_set.classes) // batch_size,\n",
        "          validation_data=val_set,\n",
        "          callbacks=[early_stop],\n",
        "          verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(f'{output_dir}/cherry_leaves_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "losses = pd.DataFrame(model.history.history)\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "losses[['loss', 'val_loss']].plot(style='.-')\n",
        "plt.title('Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig(f'{output_dir}/model_training_loss.png',\n",
        "            bbox_inches='tight', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print('\\n')\n",
        "losses[['accuracy', 'val_accuracy']].plot(style='.-')\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.savefig(f'{output_dir}/model_training_acc.png',\n",
        "            bbox_inches='tight', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(f'outputs/{version_num}/cherry_leaves_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation = model.evaluate(test_set)\n",
        "print(f'Test set evaluation reports an accuracy of {evaluation[1]} and a loss of {evaluation[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save evaluation pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joblib.dump(value=evaluation,\n",
        "            filename=f\"outputs/v1/evaluation.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "point = 40 # points at specified file index\n",
        "label = labels[1] # pick 0 for healthy and 1 for powdery_mildew\n",
        "\n",
        "img = image.load_img(test_dir + '/'+ label + '/'+ os.listdir(test_dir +'/'+ label)[point])\n",
        "img_arr = np.array(img)/255.0\n",
        "pred_proba = model.predict(np.expand_dims(img_arr, axis=0))\n",
        "\n",
        "predicted_class = 1 if pred_proba[0,0] > 0.5 else 0\n",
        "\n",
        "predicted_label = labels[predicted_class]\n",
        "\n",
        "if predicted_class == 0:\n",
        "    predicted_prob = 1 - pred_proba[0,0]\n",
        "else:\n",
        "    predicted_prob = pred_proba[0,0]\n",
        "\n",
        "print(\"Prediction is:\", predicted_label)\n",
        "print(f\"Prediction probability is: {predicted_prob*100}%\")\n",
        "print(\"Image shape is:\", img_arr.shape)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_arr = []\n",
        "\n",
        "for label in labels:\n",
        "    files_folder = test_dir + '/' + label\n",
        "    file_list = os.listdir(files_folder)\n",
        "    \n",
        "    for file in file_list:\n",
        "         file_path = os.path.join(files_folder, file)\n",
        "         label_arr.append(label)\n",
        "\n",
        "binary_labels = [0 if item == 'healthy' else 1 for item in label_arr]\n",
        "print(len(binary_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "predictions = model.predict(test_set)\n",
        "predicted_labels = (predictions > 0.5).astype(int)\n",
        "\n",
        "test_accuracy = accuracy_score(binary_labels, predicted_labels)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "actual = binary_labels\n",
        "pred = predicted_labels\n",
        "predicted = [label[0] for label in pred] # convert predicted to same format as actual label list\n",
        "\n",
        "cf_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cf_matrix, \n",
        "                                            display_labels=[labels[0], labels[1]])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# calculate specificity \n",
        "TN = cf_matrix[0,0]\n",
        "FP = cf_matrix[0,1]\n",
        "specificty = TN / (TN + FP)\n",
        "\n",
        "# calculate accuracy, precision, recall and F1 score\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "precision = precision_score(actual, predicted)\n",
        "recall = recall_score(actual, predicted)\n",
        "f1 = f1_score(actual, predicted)\n",
        "\n",
        "# create summary table\n",
        "summary_df = pd.DataFrame([{\"Accuracy:\": accuracy,\n",
        "                            \"Precision:\": precision,\n",
        "                            \"Specificity:\": specificty,\n",
        "                            \"Recall:\": recall,\n",
        "                            \"F1-score:\": f1}])\n",
        "\n",
        "summary_df.index = ['Cherry leaves model']\n",
        "summary_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ROC plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "fpr, tpr, _ = roc_curve(actual, predictions)\n",
        "roc_auc = roc_auc_score(actual, predictions)\n",
        "\n",
        "plt.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')\n",
        "plt.plot([0,1], [0,1], color='red', linestyle='--')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision recall plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "prec, recall, _ = precision_recall_curve(actual, predicted)\n",
        "area_under_curve = auc(recall, prec)\n",
        "\n",
        "plt.plot(recall, prec, color='blue', label=f'AUC = {area_under_curve:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
